
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ          ARGS           ‚îÇ PROFILE  ‚îÇ  USER  ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start      ‚îÇ --driver=docker         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 15:59 CST ‚îÇ 11 Nov 25 16:01 CST ‚îÇ
‚îÇ docker-env ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:01 CST ‚îÇ 11 Nov 25 16:01 CST ‚îÇ
‚îÇ service    ‚îÇ react-app-service --url ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:01 CST ‚îÇ                     ‚îÇ
‚îÇ dashboard  ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:01 CST ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:03 CST ‚îÇ 11 Nov 25 16:04 CST ‚îÇ
‚îÇ docker-env ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:04 CST ‚îÇ 11 Nov 25 16:04 CST ‚îÇ
‚îÇ service    ‚îÇ react-app-service --url ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:04 CST ‚îÇ                     ‚îÇ
‚îÇ dashboard  ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:04 CST ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:09 CST ‚îÇ 11 Nov 25 16:09 CST ‚îÇ
‚îÇ start      ‚îÇ --driver=docker         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:09 CST ‚îÇ 11 Nov 25 16:10 CST ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server   ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:11 CST ‚îÇ 11 Nov 25 16:11 CST ‚îÇ
‚îÇ dashboard  ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:11 CST ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                         ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:13 CST ‚îÇ 11 Nov 25 16:13 CST ‚îÇ
‚îÇ service    ‚îÇ react-app-service --url ‚îÇ minikube ‚îÇ thaovu ‚îÇ v1.37.0 ‚îÇ 11 Nov 25 16:14 CST ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/11/11 16:09:47
Running on machine: Thaos-MacBook-Pro
Binary: Built with gc go1.24.6 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1111 16:09:47.019311   30099 out.go:360] Setting OutFile to fd 1 ...
I1111 16:09:47.019593   30099 out.go:413] isatty.IsTerminal(1) = true
I1111 16:09:47.019595   30099 out.go:374] Setting ErrFile to fd 2...
I1111 16:09:47.019597   30099 out.go:413] isatty.IsTerminal(2) = true
I1111 16:09:47.019685   30099 root.go:338] Updating PATH: /Users/thaovu/.minikube/bin
I1111 16:09:47.019694   30099 oci.go:585] shell is pointing to dockerd inside minikube. will unset to use host
W1111 16:09:47.019743   30099 root.go:314] Error reading config file at /Users/thaovu/.minikube/config/config.json: open /Users/thaovu/.minikube/config/config.json: no such file or directory
I1111 16:09:47.021278   30099 out.go:368] Setting JSON to false
I1111 16:09:47.046320   30099 start.go:130] hostinfo: {"hostname":"Thaos-MacBook-Pro.local","uptime":14128,"bootTime":1762884859,"procs":555,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"26.0.1","kernelVersion":"25.0.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"df3cee09-2242-587e-9a03-5f8db5b3fa58"}
W1111 16:09:47.046589   30099 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1111 16:09:47.056468   30099 out.go:179] üòÑ  minikube v1.37.0 on Darwin 26.0.1 (arm64)
I1111 16:09:47.068034   30099 out.go:179]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I1111 16:09:47.068511   30099 notify.go:220] Checking for updates...
I1111 16:09:47.075470   30099 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1111 16:09:47.076651   30099 driver.go:421] Setting default libvirt URI to qemu:///system
I1111 16:09:47.103872   30099 docker.go:123] docker version: linux-28.5.1:Docker Desktop 4.48.0 (207573)
I1111 16:09:47.104016   30099 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1111 16:09:47.540233   30099 info.go:266] docker info: {ID:c295a898-ba8c-4694-9129-ce73df83b312 Containers:5 ContainersRunning:3 ContainersPaused:0 ContainersStopped:2 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:false NGoroutines:118 SystemTime:2025-11-11 22:09:47.521229049 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8218316800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/thaovu/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/thaovu/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-ai] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/thaovu/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:/Users/thaovu/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-cloud] ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:/Users/thaovu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2-desktop.1] map[Name:debug Path:/Users/thaovu/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:/Users/thaovu/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-desktop] ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/thaovu/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/thaovu/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/thaovu/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-mcp] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:model Path:/Users/thaovu/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-model] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:/Users/thaovu/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/thaovu/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1111 16:09:47.545193   30099 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1111 16:09:47.549140   30099 start.go:304] selected driver: docker
I1111 16:09:47.549154   30099 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1111 16:09:47.549188   30099 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1111 16:09:47.549410   30099 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1111 16:09:47.644469   30099 info.go:266] docker info: {ID:c295a898-ba8c-4694-9129-ce73df83b312 Containers:5 ContainersRunning:3 ContainersPaused:0 ContainersStopped:2 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:false NGoroutines:118 SystemTime:2025-11-11 22:09:47.630900216 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8218316800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/thaovu/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/thaovu/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-ai] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/thaovu/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:/Users/thaovu/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-cloud] ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:/Users/thaovu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2-desktop.1] map[Name:debug Path:/Users/thaovu/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:/Users/thaovu/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-desktop] ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/thaovu/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/thaovu/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/thaovu/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-mcp] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:model Path:/Users/thaovu/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-model] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:/Users/thaovu/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/thaovu/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1111 16:09:47.645205   30099 cni.go:84] Creating CNI manager for ""
I1111 16:09:47.645903   30099 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1111 16:09:47.646067   30099 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1111 16:09:47.651058   30099 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1111 16:09:47.659394   30099 cache.go:123] Beginning downloading kic base image for docker with docker
I1111 16:09:47.662968   30099 out.go:179] üöú  Pulling base image v0.0.48 ...
I1111 16:09:47.671261   30099 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1111 16:09:47.671341   30099 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1111 16:09:47.671357   30099 preload.go:146] Found local preload: /Users/thaovu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4
I1111 16:09:47.671364   30099 cache.go:58] Caching tarball of preloaded images
I1111 16:09:47.671468   30099 preload.go:172] Found /Users/thaovu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I1111 16:09:47.671472   30099 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1111 16:09:47.671712   30099 profile.go:143] Saving config to /Users/thaovu/.minikube/profiles/minikube/config.json ...
I1111 16:09:47.713776   30099 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1111 16:09:47.713809   30099 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1111 16:09:47.713818   30099 cache.go:232] Successfully downloaded all kic artifacts
I1111 16:09:47.713832   30099 start.go:360] acquireMachinesLock for minikube: {Name:mk5e270aeb2a406a6647dce45b3a3e45364a76fe Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1111 16:09:47.713916   30099 start.go:364] duration metric: took 75.667¬µs to acquireMachinesLock for "minikube"
I1111 16:09:47.713940   30099 start.go:96] Skipping create...Using existing machine configuration
I1111 16:09:47.713942   30099 fix.go:54] fixHost starting: 
I1111 16:09:47.718181   30099 out.go:179] üìå  Noticed you have an activated docker-env on docker driver in this terminal:
W1111 16:09:47.722200   30099 out.go:285] ‚ùó  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I1111 16:09:47.722277   30099 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1111 16:09:47.740044   30099 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1111 16:09:47.740080   30099 fix.go:138] unexpected machine state, will restart: <nil>
I1111 16:09:47.744240   30099 out.go:252] üèÉ  Updating the running docker "minikube" container ...
I1111 16:09:47.744422   30099 machine.go:93] provisionDockerMachine start ...
I1111 16:09:47.744506   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:47.762911   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:47.763229   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:47.763233   30099 main.go:141] libmachine: About to run SSH command:
hostname
I1111 16:09:47.896564   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1111 16:09:47.896813   30099 ubuntu.go:182] provisioning hostname "minikube"
I1111 16:09:47.896920   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:47.922406   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:47.922614   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:47.922620   30099 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1111 16:09:48.052416   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1111 16:09:48.052545   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:48.077378   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:48.077630   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:48.077640   30099 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1111 16:09:48.201457   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1111 16:09:48.201493   30099 ubuntu.go:188] set auth options {CertDir:/Users/thaovu/.minikube CaCertPath:/Users/thaovu/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/thaovu/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/thaovu/.minikube/machines/server.pem ServerKeyPath:/Users/thaovu/.minikube/machines/server-key.pem ClientKeyPath:/Users/thaovu/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/thaovu/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/thaovu/.minikube}
I1111 16:09:48.201533   30099 ubuntu.go:190] setting up certificates
I1111 16:09:48.201546   30099 provision.go:84] configureAuth start
I1111 16:09:48.201766   30099 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1111 16:09:48.237091   30099 provision.go:143] copyHostCerts
I1111 16:09:48.237489   30099 exec_runner.go:144] found /Users/thaovu/.minikube/ca.pem, removing ...
I1111 16:09:48.237499   30099 exec_runner.go:203] rm: /Users/thaovu/.minikube/ca.pem
I1111 16:09:48.237878   30099 exec_runner.go:151] cp: /Users/thaovu/.minikube/certs/ca.pem --> /Users/thaovu/.minikube/ca.pem (1078 bytes)
I1111 16:09:48.238365   30099 exec_runner.go:144] found /Users/thaovu/.minikube/cert.pem, removing ...
I1111 16:09:48.238368   30099 exec_runner.go:203] rm: /Users/thaovu/.minikube/cert.pem
I1111 16:09:48.238652   30099 exec_runner.go:151] cp: /Users/thaovu/.minikube/certs/cert.pem --> /Users/thaovu/.minikube/cert.pem (1119 bytes)
I1111 16:09:48.238992   30099 exec_runner.go:144] found /Users/thaovu/.minikube/key.pem, removing ...
I1111 16:09:48.238996   30099 exec_runner.go:203] rm: /Users/thaovu/.minikube/key.pem
I1111 16:09:48.239131   30099 exec_runner.go:151] cp: /Users/thaovu/.minikube/certs/key.pem --> /Users/thaovu/.minikube/key.pem (1675 bytes)
I1111 16:09:48.239395   30099 provision.go:117] generating server cert: /Users/thaovu/.minikube/machines/server.pem ca-key=/Users/thaovu/.minikube/certs/ca.pem private-key=/Users/thaovu/.minikube/certs/ca-key.pem org=thaovu.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1111 16:09:48.357585   30099 provision.go:177] copyRemoteCerts
I1111 16:09:48.357657   30099 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1111 16:09:48.357697   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:48.379456   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:09:48.476167   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1111 16:09:48.493803   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1111 16:09:48.507816   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1111 16:09:48.519683   30099 provision.go:87] duration metric: took 318.12925ms to configureAuth
I1111 16:09:48.519690   30099 ubuntu.go:206] setting minikube options for container-runtime
I1111 16:09:48.519824   30099 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1111 16:09:48.519898   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:48.540927   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:48.541117   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:48.541121   30099 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1111 16:09:48.664690   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1111 16:09:48.664749   30099 ubuntu.go:71] root file system type: overlay
I1111 16:09:48.664867   30099 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1111 16:09:48.665049   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:48.698219   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:48.698525   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:48.698591   30099 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1111 16:09:48.836328   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1111 16:09:48.836491   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:48.863495   30099 main.go:141] libmachine: Using SSH client type: native
I1111 16:09:48.863718   30099 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102e937c0] 0x102e95f80 <nil>  [] 0s} 127.0.0.1 50812 <nil> <nil>}
I1111 16:09:48.863727   30099 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1111 16:09:48.990604   30099 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1111 16:09:48.990635   30099 machine.go:96] duration metric: took 1.246193708s to provisionDockerMachine
I1111 16:09:48.990652   30099 start.go:293] postStartSetup for "minikube" (driver="docker")
I1111 16:09:48.990661   30099 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1111 16:09:48.990830   30099 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1111 16:09:48.990914   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:49.017420   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:09:49.110330   30099 ssh_runner.go:195] Run: cat /etc/os-release
I1111 16:09:49.112097   30099 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1111 16:09:49.112128   30099 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1111 16:09:49.112135   30099 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1111 16:09:49.112139   30099 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1111 16:09:49.112146   30099 filesync.go:126] Scanning /Users/thaovu/.minikube/addons for local assets ...
I1111 16:09:49.112282   30099 filesync.go:126] Scanning /Users/thaovu/.minikube/files for local assets ...
I1111 16:09:49.112346   30099 start.go:296] duration metric: took 121.688625ms for postStartSetup
I1111 16:09:49.112425   30099 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1111 16:09:49.112484   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:49.136101   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:09:49.224644   30099 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1111 16:09:49.227912   30099 fix.go:56] duration metric: took 1.513954375s for fixHost
I1111 16:09:49.227922   30099 start.go:83] releasing machines lock for "minikube", held for 1.513988791s
I1111 16:09:49.228030   30099 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1111 16:09:49.263177   30099 ssh_runner.go:195] Run: cat /version.json
I1111 16:09:49.263277   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:49.264039   30099 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1111 16:09:49.264253   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:09:49.290308   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:09:49.290312   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:09:49.614537   30099 ssh_runner.go:195] Run: systemctl --version
I1111 16:09:49.618832   30099 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1111 16:09:49.622791   30099 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1111 16:09:49.636421   30099 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1111 16:09:49.636511   30099 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1111 16:09:49.642009   30099 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1111 16:09:49.642026   30099 start.go:495] detecting cgroup driver to use...
I1111 16:09:49.642048   30099 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1111 16:09:49.643694   30099 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1111 16:09:49.652203   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1111 16:09:49.657064   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1111 16:09:49.661551   30099 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1111 16:09:49.661634   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1111 16:09:49.666262   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1111 16:09:49.670654   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1111 16:09:49.674750   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1111 16:09:49.678558   30099 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1111 16:09:49.682370   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1111 16:09:49.686374   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1111 16:09:49.690171   30099 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1111 16:09:49.693978   30099 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1111 16:09:49.697465   30099 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1111 16:09:49.700828   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:09:49.748546   30099 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1111 16:09:49.879317   30099 start.go:495] detecting cgroup driver to use...
I1111 16:09:49.879345   30099 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1111 16:09:49.879520   30099 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1111 16:09:49.884809   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1111 16:09:49.889559   30099 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1111 16:09:49.900291   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1111 16:09:49.905171   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1111 16:09:49.909936   30099 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1111 16:09:49.915923   30099 ssh_runner.go:195] Run: which cri-dockerd
I1111 16:09:49.917435   30099 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1111 16:09:49.920744   30099 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1111 16:09:49.927515   30099 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1111 16:09:49.972638   30099 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1111 16:09:50.016148   30099 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1111 16:09:50.016247   30099 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1111 16:09:50.023312   30099 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1111 16:09:50.027632   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:09:50.072642   30099 ssh_runner.go:195] Run: sudo systemctl restart docker
I1111 16:10:16.306577   30099 ssh_runner.go:235] Completed: sudo systemctl restart docker: (26.233695292s)
I1111 16:10:16.306776   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1111 16:10:16.313656   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1111 16:10:16.320076   30099 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1111 16:10:16.330189   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1111 16:10:16.363113   30099 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1111 16:10:16.397521   30099 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1111 16:10:16.429946   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:10:16.462538   30099 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1111 16:10:16.488299   30099 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1111 16:10:16.492877   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:10:16.524970   30099 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1111 16:10:16.574418   30099 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1111 16:10:16.579335   30099 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1111 16:10:16.580796   30099 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1111 16:10:16.582746   30099 start.go:563] Will wait 60s for crictl version
I1111 16:10:16.582948   30099 ssh_runner.go:195] Run: which crictl
I1111 16:10:16.584661   30099 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1111 16:10:16.602306   30099 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1111 16:10:16.602494   30099 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1111 16:10:16.615672   30099 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1111 16:10:16.630891   30099 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1111 16:10:16.631061   30099 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1111 16:10:16.712643   30099 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1111 16:10:16.713703   30099 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1111 16:10:16.715917   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1111 16:10:16.737398   30099 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1111 16:10:16.737528   30099 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1111 16:10:16.737623   30099 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1111 16:10:16.746522   30099 docker.go:691] Got preloaded images: -- stdout --
react-app:latest
<none>:<none>
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1111 16:10:16.746529   30099 docker.go:621] Images already preloaded, skipping extraction
I1111 16:10:16.746833   30099 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1111 16:10:16.754221   30099 docker.go:691] Got preloaded images: -- stdout --
<none>:<none>
react-app:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1111 16:10:16.754227   30099 cache_images.go:85] Images are preloaded, skipping loading
I1111 16:10:16.754233   30099 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1111 16:10:16.754580   30099 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1111 16:10:16.754632   30099 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1111 16:10:16.780825   30099 cni.go:84] Creating CNI manager for ""
I1111 16:10:16.780835   30099 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1111 16:10:16.780841   30099 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1111 16:10:16.780854   30099 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1111 16:10:16.780925   30099 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1111 16:10:16.781030   30099 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1111 16:10:16.784797   30099 binaries.go:44] Found k8s binaries, skipping transfer
I1111 16:10:16.784863   30099 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1111 16:10:16.788231   30099 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1111 16:10:16.794835   30099 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1111 16:10:16.801302   30099 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1111 16:10:16.807988   30099 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1111 16:10:16.809670   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:10:16.841899   30099 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1111 16:10:16.848053   30099 certs.go:68] Setting up /Users/thaovu/.minikube/profiles/minikube for IP: 192.168.49.2
I1111 16:10:16.848068   30099 certs.go:194] generating shared ca certs ...
I1111 16:10:16.848094   30099 certs.go:226] acquiring lock for ca certs: {Name:mkd5b386b9423889a03bf5eba86be0be1336a515 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1111 16:10:16.848903   30099 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/thaovu/.minikube/ca.key
I1111 16:10:16.849299   30099 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/thaovu/.minikube/proxy-client-ca.key
I1111 16:10:16.849311   30099 certs.go:256] generating profile certs ...
I1111 16:10:16.849947   30099 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/thaovu/.minikube/profiles/minikube/client.key
I1111 16:10:16.850438   30099 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/thaovu/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1111 16:10:16.850931   30099 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/thaovu/.minikube/profiles/minikube/proxy-client.key
I1111 16:10:16.851420   30099 certs.go:484] found cert: /Users/thaovu/.minikube/certs/ca-key.pem (1679 bytes)
I1111 16:10:16.851675   30099 certs.go:484] found cert: /Users/thaovu/.minikube/certs/ca.pem (1078 bytes)
I1111 16:10:16.851858   30099 certs.go:484] found cert: /Users/thaovu/.minikube/certs/cert.pem (1119 bytes)
I1111 16:10:16.852030   30099 certs.go:484] found cert: /Users/thaovu/.minikube/certs/key.pem (1675 bytes)
I1111 16:10:16.853740   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1111 16:10:16.862381   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1111 16:10:16.870684   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1111 16:10:16.879142   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1111 16:10:16.888413   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1111 16:10:16.896684   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1111 16:10:16.904989   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1111 16:10:16.913533   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1111 16:10:16.922713   30099 ssh_runner.go:362] scp /Users/thaovu/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1111 16:10:16.930939   30099 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1111 16:10:16.937403   30099 ssh_runner.go:195] Run: openssl version
I1111 16:10:16.940006   30099 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1111 16:10:16.943821   30099 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1111 16:10:16.945469   30099 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 11 22:01 /usr/share/ca-certificates/minikubeCA.pem
I1111 16:10:16.945581   30099 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1111 16:10:16.948685   30099 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1111 16:10:16.952311   30099 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1111 16:10:16.953923   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1111 16:10:16.956728   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1111 16:10:16.959408   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1111 16:10:16.962854   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1111 16:10:16.965835   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1111 16:10:16.968877   30099 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1111 16:10:16.971819   30099 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1111 16:10:16.972088   30099 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1111 16:10:16.978944   30099 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1111 16:10:16.982519   30099 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1111 16:10:16.982803   30099 kubeadm.go:589] restartPrimaryControlPlane start ...
I1111 16:10:16.982948   30099 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1111 16:10:16.986122   30099 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1111 16:10:16.986256   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1111 16:10:17.015422   30099 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:50811"
I1111 16:10:17.027187   30099 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1111 16:10:17.031342   30099 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1111 16:10:17.031355   30099 kubeadm.go:593] duration metric: took 48.547167ms to restartPrimaryControlPlane
I1111 16:10:17.031359   30099 kubeadm.go:394] duration metric: took 59.56425ms to StartCluster
I1111 16:10:17.031367   30099 settings.go:142] acquiring lock: {Name:mkb9935816724f836c5c94920b4d5cab5c451c7f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1111 16:10:17.031437   30099 settings.go:150] Updating kubeconfig:  /Users/thaovu/.kube/config
I1111 16:10:17.031719   30099 lock.go:35] WriteFile acquiring /Users/thaovu/.kube/config: {Name:mk7c2de6f09854b8be5e876ebbec74d8f16a8bfc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1111 16:10:17.032218   30099 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1111 16:10:17.032287   30099 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1111 16:10:17.032463   30099 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1111 16:10:17.034557   30099 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1111 16:10:17.034571   30099 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1111 16:10:17.034570   30099 addons.go:69] Setting dashboard=true in profile "minikube"
W1111 16:10:17.034576   30099 addons.go:247] addon storage-provisioner should already be in state true
I1111 16:10:17.034580   30099 addons.go:238] Setting addon dashboard=true in "minikube"
W1111 16:10:17.034583   30099 addons.go:247] addon dashboard should already be in state true
I1111 16:10:17.034587   30099 host.go:66] Checking if "minikube" exists ...
I1111 16:10:17.034598   30099 host.go:66] Checking if "minikube" exists ...
I1111 16:10:17.034602   30099 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1111 16:10:17.034618   30099 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1111 16:10:17.034957   30099 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1111 16:10:17.034997   30099 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1111 16:10:17.035010   30099 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1111 16:10:17.052481   30099 out.go:179] üîé  Verifying Kubernetes components...
I1111 16:10:17.057669   30099 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1111 16:10:17.060492   30099 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1111 16:10:17.060498   30099 addons.go:247] addon default-storageclass should already be in state true
I1111 16:10:17.060512   30099 host.go:66] Checking if "minikube" exists ...
I1111 16:10:17.060773   30099 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1111 16:10:17.064552   30099 out.go:179]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1111 16:10:17.064552   30099 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1111 16:10:17.068019   30099 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1111 16:10:17.068023   30099 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1111 16:10:17.068087   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:10:17.072026   30099 out.go:179]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1111 16:10:17.076233   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1111 16:10:17.076241   30099 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1111 16:10:17.076343   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:10:17.087683   30099 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:17.087720   30099 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1111 16:10:17.087894   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1111 16:10:17.093006   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:10:17.100086   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:10:17.103637   30099 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1111 16:10:17.108224   30099 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50812 SSHKeyPath:/Users/thaovu/.minikube/machines/minikube/id_rsa Username:docker}
I1111 16:10:17.110401   30099 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1111 16:10:17.127543   30099 api_server.go:52] waiting for apiserver process to appear ...
I1111 16:10:17.127627   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:17.180758   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1111 16:10:17.185186   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1111 16:10:17.185191   30099 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1111 16:10:17.192868   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:17.192856   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1111 16:10:17.192874   30099 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1111 16:10:17.200366   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1111 16:10:17.200376   30099 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1111 16:10:17.207725   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1111 16:10:17.207730   30099 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1111 16:10:17.216461   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I1111 16:10:17.216466   30099 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
W1111 16:10:17.216654   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.216671   30099 retry.go:31] will retry after 347.96824ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1111 16:10:17.219855   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.219861   30099 retry.go:31] will retry after 246.657575ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.223296   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1111 16:10:17.223300   30099 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1111 16:10:17.229689   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1111 16:10:17.229694   30099 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1111 16:10:17.236501   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1111 16:10:17.236504   30099 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1111 16:10:17.242768   30099 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1111 16:10:17.242771   30099 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1111 16:10:17.249158   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:17.272884   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.272894   30099 retry.go:31] will retry after 220.420913ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.467559   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:17.493837   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:17.563157   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.563180   30099 retry.go:31] will retry after 246.627569ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.565088   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1111 16:10:17.628984   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1111 16:10:17.682638   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.682675   30099 retry.go:31] will retry after 216.56523ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1111 16:10:17.760853   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.760876   30099 retry.go:31] will retry after 384.486492ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.811280   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:17.900217   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:17.972658   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:17.972678   30099 retry.go:31] will retry after 409.001551ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1111 16:10:18.062635   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.062661   30099 retry.go:31] will retry after 388.485262ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.129040   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:18.146740   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:18.193776   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.193796   30099 retry.go:31] will retry after 529.054476ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.383013   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:18.452640   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:18.459796   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.459817   30099 retry.go:31] will retry after 623.096023ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1111 16:10:18.490402   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.490423   30099 retry.go:31] will retry after 490.312156ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.628134   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:18.723419   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:18.762861   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.762876   30099 retry.go:31] will retry after 461.098503ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:18.982128   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:19.008192   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.008215   30099 retry.go:31] will retry after 854.110935ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.084293   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1111 16:10:19.109354   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.109368   30099 retry.go:31] will retry after 656.778591ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.128495   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:19.225237   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:19.251791   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.251801   30099 retry.go:31] will retry after 1.154078783s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.628954   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:19.766629   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1111 16:10:19.795845   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.795870   30099 retry.go:31] will retry after 1.226561348s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.863649   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:19.894302   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:19.894336   30099 retry.go:31] will retry after 2.44373189s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:20.129300   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:20.407594   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:20.464095   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:20.464125   30099 retry.go:31] will retry after 1.435431151s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:20.628371   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:21.024086   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1111 16:10:21.078152   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:21.078180   30099 retry.go:31] will retry after 3.721411352s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:21.129003   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:21.629307   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:21.901260   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:21.957190   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:21.957221   30099 retry.go:31] will retry after 3.84096167s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:22.128619   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:22.339382   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:22.389399   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:22.389412   30099 retry.go:31] will retry after 2.345331389s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:22.628776   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:23.128128   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:23.629207   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:24.129397   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:24.629192   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:24.736447   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:24.791477   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:24.791523   30099 retry.go:31] will retry after 4.449419903s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:24.800560   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1111 16:10:24.829438   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:24.829458   30099 retry.go:31] will retry after 3.353847564s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:25.129424   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:25.628157   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:25.799757   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:25.855650   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:25.855677   30099 retry.go:31] will retry after 2.632927571s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:26.128737   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:26.629362   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:27.129257   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:27.629410   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:28.129088   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:28.184839   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1111 16:10:28.239175   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:28.239202   30099 retry.go:31] will retry after 5.856649983s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:28.490147   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1111 16:10:28.547555   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:28.547573   30099 retry.go:31] will retry after 7.485904113s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:28.629383   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:29.129172   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:29.242275   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1111 16:10:29.285965   30099 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:29.285993   30099 retry.go:31] will retry after 9.186584686s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1111 16:10:29.628788   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:30.129176   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:30.629088   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:31.129377   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:31.629000   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:32.129266   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:32.629008   30099 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1111 16:10:32.634320   30099 api_server.go:72] duration metric: took 15.601889334s to wait for apiserver process to appear ...
I1111 16:10:32.634341   30099 api_server.go:88] waiting for apiserver healthz status ...
I1111 16:10:32.634356   30099 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50811/healthz ...
I1111 16:10:33.261008   30099 api_server.go:279] https://127.0.0.1:50811/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1111 16:10:33.261025   30099 api_server.go:103] status: https://127.0.0.1:50811/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1111 16:10:33.261038   30099 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50811/healthz ...
I1111 16:10:33.273233   30099 api_server.go:279] https://127.0.0.1:50811/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1111 16:10:33.273247   30099 api_server.go:103] status: https://127.0.0.1:50811/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1111 16:10:33.635491   30099 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50811/healthz ...
I1111 16:10:33.645620   30099 api_server.go:279] https://127.0.0.1:50811/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1111 16:10:33.645631   30099 api_server.go:103] status: https://127.0.0.1:50811/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1111 16:10:34.097478   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1111 16:10:34.135553   30099 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50811/healthz ...
I1111 16:10:34.141817   30099 api_server.go:279] https://127.0.0.1:50811/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1111 16:10:34.141833   30099 api_server.go:103] status: https://127.0.0.1:50811/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1111 16:10:34.635542   30099 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50811/healthz ...
I1111 16:10:34.648058   30099 api_server.go:279] https://127.0.0.1:50811/healthz returned 200:
ok
I1111 16:10:34.650249   30099 api_server.go:141] control plane version: v1.34.0
I1111 16:10:34.650272   30099 api_server.go:131] duration metric: took 2.015907167s to wait for apiserver health ...
I1111 16:10:34.651254   30099 system_pods.go:43] waiting for kube-system pods to appear ...
I1111 16:10:34.660727   30099 system_pods.go:59] 7 kube-system pods found
I1111 16:10:34.660754   30099 system_pods.go:61] "coredns-66bc5c9577-dm8tk" [4bab151e-d387-4caf-922d-9417afaf4e0a] Running
I1111 16:10:34.660761   30099 system_pods.go:61] "etcd-minikube" [78240a77-b9d3-417c-bea9-d67a9f62cff4] Running
I1111 16:10:34.660766   30099 system_pods.go:61] "kube-apiserver-minikube" [6627eea5-2ddc-4ce0-9da7-f979f173064b] Running
I1111 16:10:34.660771   30099 system_pods.go:61] "kube-controller-manager-minikube" [55a2515c-c86e-469f-b5d4-de0271dad21e] Running
I1111 16:10:34.660775   30099 system_pods.go:61] "kube-proxy-pm885" [c40451cb-c1d4-4ef9-9f8a-f6375892588b] Running
I1111 16:10:34.660780   30099 system_pods.go:61] "kube-scheduler-minikube" [e9e8bf4e-8cf4-4eb9-8a8d-054512e4e609] Running
I1111 16:10:34.660787   30099 system_pods.go:61] "storage-provisioner" [497e9acd-22a0-42c5-8e46-d8e111524300] Running
I1111 16:10:34.660794   30099 system_pods.go:74] duration metric: took 9.531667ms to wait for pod list to return data ...
I1111 16:10:34.660816   30099 kubeadm.go:578] duration metric: took 17.628359375s to wait for: map[apiserver:true system_pods:true]
I1111 16:10:34.660835   30099 node_conditions.go:102] verifying NodePressure condition ...
I1111 16:10:34.666389   30099 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1111 16:10:34.666408   30099 node_conditions.go:123] node cpu capacity is 8
I1111 16:10:34.666425   30099 node_conditions.go:105] duration metric: took 5.585542ms to run NodePressure ...
I1111 16:10:34.666441   30099 start.go:241] waiting for startup goroutines ...
I1111 16:10:36.034373   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1111 16:10:38.473249   30099 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1111 16:10:39.068679   30099 out.go:179] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I1111 16:10:39.074587   30099 out.go:179] üåü  Enabled addons: default-storageclass, storage-provisioner, dashboard
I1111 16:10:39.091851   30099 addons.go:514] duration metric: took 22.0592005s for enable addons: enabled=[default-storageclass storage-provisioner dashboard]
I1111 16:10:39.091884   30099 start.go:246] waiting for cluster config update ...
I1111 16:10:39.091894   30099 start.go:255] writing updated cluster config ...
I1111 16:10:39.098229   30099 ssh_runner.go:195] Run: rm -f paused
I1111 16:10:39.303107   30099 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1111 16:10:39.307607   30099 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.273337966Z" level=info msg="Initializing buildkit"
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.299378882Z" level=info msg="Completed buildkit initialization"
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.301503674Z" level=info msg="Daemon has completed initialization"
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.301539341Z" level=info msg="API listen on /var/run/docker.sock"
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.301621757Z" level=info msg="API listen on [::]:2376"
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.301646299Z" level=info msg="API listen on /run/docker.sock"
Nov 11 22:10:16 minikube systemd[1]: Started Docker Application Container Engine.
Nov 11 22:10:16 minikube cri-dockerd[5397]: time="2025-11-11T22:10:16Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-77bf4d6c4c-wlq4h_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4f3c7b60bcf7c77d27c0433c3b617e282423e86b56891b1f3d9538f0debf0e82\""
Nov 11 22:10:16 minikube cri-dockerd[5397]: time="2025-11-11T22:10:16Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"react-app-55f69d4ddc-gpk2n_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a28446bb3dbe4088d36b19bcce5d00d6a4919457f470155e541903c35e3bfadd\""
Nov 11 22:10:16 minikube cri-dockerd[5397]: time="2025-11-11T22:10:16Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"react-app-55f69d4ddc-ddlgz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e7c16791e43f3836a0ce3f4d84f47403254d22168b004fceccfc185c2b5ae44e\""
Nov 11 22:10:16 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10154 (firewall) remains running after unit stopped.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10159 (firewall) remains running after unit stopped.
Nov 11 22:10:16 minikube dockerd[9307]: time="2025-11-11T22:10:16.326597674Z" level=error msg="Handler for GET /v1.46/containers/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Nov 11 22:10:16 minikube dockerd[9307]: 2025/11/11 22:10:16 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:83)
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10166 (iptables) remains running after unit stopped.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10169 (iptables) remains running after unit stopped.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10176 (firewall) remains running after unit stopped.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Unit process 10181 (iptables) remains running after unit stopped.
Nov 11 22:10:16 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Nov 11 22:10:16 minikube systemd[1]: cri-docker.service: Consumed 5.345s CPU time.
Nov 11 22:10:16 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Start docker client with request timeout 0s"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Loaded network plugin cni"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 11 22:10:16 minikube cri-dockerd[10262]: time="2025-11-11T22:10:16Z" level=info msg="Start cri-dockerd grpc backend"
Nov 11 22:10:16 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-77bf4d6c4c-wlq4h_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4f3c7b60bcf7c77d27c0433c3b617e282423e86b56891b1f3d9538f0debf0e82\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-77bf4d6c4c-wlq4h_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c78f8f80d319e0b333fc1d14ab32e8397f0513da831fe970d5ad07e156be2d2d\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-dm8tk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8e811df9cc2e135eb84247aa34a2194711e3bd227d73b73e762f26e2ce958220\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-dm8tk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"eea3969973089222480ce5772206edefd2eac7fea49cafbd4a8e13a2aa9b1b2c\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-855c9754f9-bpqz5_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0f3f5060e82e2e6c55cbe118d3a1111e1de50d216e97bbe26f293fed7f4a732d\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-855c9754f9-bpqz5_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"18d15d662421188f2b3a358904273f9c7c9f606150604ae2bc3b87d25a006429\""
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/70a13472f8948bc2327804f97cce75716b9f8eae75b7f952a52c6fdef72df929/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6c5ba2f63246c750a08fcb0542f9dde05b9716c272b7f6ae36f4a6d0822192b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b00124d0023a6f0ce95ada99be0807827135782c27de741a5ee96f6ef37ff07b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e656db5ea05434ab3096ec997f46bece5fa17cf95fc926afba99af4c9a137f04/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d3d7c565ce14577a4cd4d9faaf6cb5fc14b1d91bece02d45dbd6adcc45b82d4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb5ef486053106b6b3a4bd0223f2f569a177b8d53bc6f62064e14cc3b50815d0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/62425a7b4e0d958823363437e2780dd969b43a952344806aef3d8ba8d86642b5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f2ac83f3e1f8685c7467e019c463d5c2c068ce81e0c51583618fba0958abb451/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2af3b6070932ad2d4d44b43aaa8b28da6bf276e6cef757201bb640c80aa6707f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc296af419b46af351f58b8350c218f39484b1c58e200ac8d3410760c41d71e3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 11 22:10:17 minikube cri-dockerd[10262]: time="2025-11-11T22:10:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f0a18ff371c23e1e4e16454408f6d8f572756b2af9ebf05fcb0eb828af246ff/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 11 22:10:18 minikube dockerd[9307]: time="2025-11-11T22:10:18.168618341Z" level=info msg="ignoring event" container=3b93af034a062028a6f99f51339ddbc7786a3a073a441ebdb1517b340d770825 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 11 22:10:29 minikube dockerd[9307]: time="2025-11-11T22:10:29.243398013Z" level=info msg="ignoring event" container=82be0dc63477ad9efaaf93e2fd77a36a2b6f7ec99a6f12c3eea1066506ea4880 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 11 22:11:36 minikube cri-dockerd[10262]: time="2025-11-11T22:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bead451d9495dcaac1bf7de1d1acaa17b1fa705e4a7342615c9b7bb38b1e3a61/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 11 22:11:37 minikube dockerd[9307]: time="2025-11-11T22:11:37.190851795Z" level=warning msg="reference for unknown type: " digest="sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2" remote="registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2"
Nov 11 22:11:45 minikube cri-dockerd[10262]: time="2025-11-11T22:11:45Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.8.0@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2"
Nov 11 22:14:53 minikube dockerd[9307]: time="2025-11-11T22:14:53.596697886Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 11 22:14:53 minikube dockerd[9307]: time="2025-11-11T22:14:53.596899136Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 11 22:15:00 minikube dockerd[9307]: time="2025-11-11T22:15:00.467987916Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 11 22:15:00 minikube dockerd[9307]: time="2025-11-11T22:15:00.468135916Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
9543a7f92a9e6       registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2   3 minutes ago       Running             metrics-server              0                   bead451d9495d       metrics-server-85b7d694d7-d62gq
0d8a06bcd9b1c       996be7e86d9b3                                                                                                           4 minutes ago       Running             kube-controller-manager     3                   a6c5ba2f63246       kube-controller-manager-minikube
83dbd999e7f03       ba04bb24b9575                                                                                                           4 minutes ago       Running             storage-provisioner         4                   b00124d0023a6       storage-provisioner
d860c19805d4f       20b332c9a70d8                                                                                                           4 minutes ago       Running             kubernetes-dashboard        3                   7f0a18ff371c2       kubernetes-dashboard-855c9754f9-bpqz5
050dfaa93ff26       d291939e99406                                                                                                           4 minutes ago       Running             kube-apiserver              2                   5d3d7c565ce14       kube-apiserver-minikube
c7a84feba6a95       a1894772a478e                                                                                                           4 minutes ago       Running             etcd                        2                   bb5ef48605310       etcd-minikube
9ec427e869365       138784d87c9c5                                                                                                           4 minutes ago       Running             coredns                     2                   2af3b6070932a       coredns-66bc5c9577-dm8tk
d965ca2bf5d0d       6fc32d66c1411                                                                                                           4 minutes ago       Running             kube-proxy                  2                   70a13472f8948       kube-proxy-pm885
3b93af034a062       20b332c9a70d8                                                                                                           4 minutes ago       Exited              kubernetes-dashboard        2                   7f0a18ff371c2       kubernetes-dashboard-855c9754f9-bpqz5
a535d193ffe52       a422e0e982356                                                                                                           4 minutes ago       Running             dashboard-metrics-scraper   2                   f2ac83f3e1f86       dashboard-metrics-scraper-77bf4d6c4c-wlq4h
82be0dc63477a       996be7e86d9b3                                                                                                           4 minutes ago       Exited              kube-controller-manager     2                   a6c5ba2f63246       kube-controller-manager-minikube
d737346b8206f       a25f5ef9c34c3                                                                                                           4 minutes ago       Running             kube-scheduler              2                   e656db5ea0543       kube-scheduler-minikube
a3390aa677aeb       ba04bb24b9575                                                                                                           10 minutes ago      Exited              storage-provisioner         3                   792b4b282530e       storage-provisioner
9ca019e420fb2       a422e0e982356                                                                                                           11 minutes ago      Exited              dashboard-metrics-scraper   1                   4f3c7b60bcf7c       dashboard-metrics-scraper-77bf4d6c4c-wlq4h
faeb842f9be62       138784d87c9c5                                                                                                           11 minutes ago      Exited              coredns                     1                   8e811df9cc2e1       coredns-66bc5c9577-dm8tk
04e6fd8c64cbe       6fc32d66c1411                                                                                                           11 minutes ago      Exited              kube-proxy                  1                   ecd0ec2fdc125       kube-proxy-pm885
3887ff7c1c5a4       a25f5ef9c34c3                                                                                                           11 minutes ago      Exited              kube-scheduler              1                   4fe857b8e90d0       kube-scheduler-minikube
119281963ddab       a1894772a478e                                                                                                           11 minutes ago      Exited              etcd                        1                   ae656e21fc3fa       etcd-minikube
8c25250462eb8       d291939e99406                                                                                                           11 minutes ago      Exited              kube-apiserver              1                   cd0019c3c532b       kube-apiserver-minikube


==> coredns [9ec427e86936] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/arm64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:51148 - 9660 "HINFO IN 4901154573794510781.5019100072151762188. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.113318958s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [faeb842f9be6] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: namespaces is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "namespaces" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:coredns" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:service-account-issuer-discovery" not found]
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: services is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "services" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:coredns" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:service-account-issuer-discovery" not found]
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpointslices" in API group "discovery.k8s.io" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:coredns" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:service-account-issuer-discovery" not found]
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/arm64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:50682 - 2842 "HINFO IN 4676611529175854023.1327788838719316650. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.080323666s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_11T16_01_38_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 11 Nov 2025 22:01:35 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Nov 2025 22:15:10 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Nov 2025 22:14:20 +0000   Tue, 11 Nov 2025 22:01:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Nov 2025 22:14:20 +0000   Tue, 11 Nov 2025 22:01:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Nov 2025 22:14:20 +0000   Tue, 11 Nov 2025 22:01:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Nov 2025 22:14:20 +0000   Tue, 11 Nov 2025 22:01:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025700Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025700Ki
  pods:               110
System Info:
  Machine ID:                 39aa54a2ae9e408084217858c4b2e284
  System UUID:                39aa54a2ae9e408084217858c4b2e284
  Boot ID:                    83c47f15-2825-46c1-8ad0-321ba05772dd
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     react-app-55f69d4ddc-ddlgz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  default                     react-app-55f69d4ddc-gpk2n                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 coredns-66bc5c9577-dm8tk                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     13m
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         13m
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-proxy-pm885                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 metrics-server-85b7d694d7-d62gq               100m (1%)     0 (0%)      200Mi (2%)       0 (0%)         3m37s
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kubernetes-dashboard        dashboard-metrics-scraper-77bf4d6c4c-wlq4h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kubernetes-dashboard        kubernetes-dashboard-855c9754f9-bpqz5         0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             370Mi (4%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 13m                kube-proxy       
  Normal   Starting                 4m35s              kube-proxy       
  Normal   Starting                 11m                kube-proxy       
  Normal   Starting                 13m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  13m (x8 over 13m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13m (x8 over 13m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13m (x7 over 13m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13m                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 13m                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  13m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  13m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           13m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed        11m                kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   RegisteredNode           11m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           4m19s              node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov11 20:49] pci-host-generic 40000000.pci: Memory resource size exceeds max for 32 bits
[  +0.271062] netlink: 'init': attribute type 4 has an invalid length.
[  +0.080794] fakeowner: loading out-of-tree module taints kernel.
[  +1.534542] netlink: 'init': attribute type 22 has an invalid length.


==> etcd [119281963dda] <==
{"level":"warn","ts":"2025-11-11T22:03:58.990395Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55606","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.065344Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55618","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.068873Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55634","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.072378Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55652","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.078831Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.082454Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55706","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.086046Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.088830Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.091631Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55778","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.094929Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55798","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.097902Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55820","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.157578Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55832","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.161339Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55842","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.164803Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55858","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.168985Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.173854Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55882","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.177249Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55900","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.180935Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55916","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.184972Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55932","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.188816Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55944","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.194022Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55974","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.258662Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55986","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.263894Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56006","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.270709Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56018","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.273942Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56042","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.278232Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56064","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.286876Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56088","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.290183Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56112","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.293787Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.360375Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56132","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.365175Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56150","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.368952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56162","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.372865Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56198","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.376245Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56204","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.379939Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56220","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.383289Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56234","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.386766Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56252","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.391436Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56268","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.460408Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56286","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.464459Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56308","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.470551Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56328","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:03:59.558053Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56350","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-11T22:10:05.310561Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-11-11T22:10:05.310737Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2025-11-11T22:10:05.311076Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-11-11T22:10:12.317736Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-11-11T22:10:12.319041Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2381: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-11-11T22:10:12.319145Z","caller":"etcdserver/server.go:1281","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-11-11T22:10:12.319345Z","caller":"etcdserver/server.go:2319","msg":"server has stopped; stopping cluster version's monitor"}
{"level":"info","ts":"2025-11-11T22:10:12.319383Z","caller":"etcdserver/server.go:2342","msg":"server has stopped; stopping storage version's monitor"}
{"level":"warn","ts":"2025-11-11T22:10:12.320219Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-11-11T22:10:12.320287Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2025-11-11T22:10:12.320340Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"warn","ts":"2025-11-11T22:10:12.321503Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-11-11T22:10:12.321534Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"error","ts":"2025-11-11T22:10:12.321612Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-11-11T22:10:12.325027Z","caller":"embed/etcd.go:621","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"error","ts":"2025-11-11T22:10:12.325130Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2380: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-11-11T22:10:12.325198Z","caller":"embed/etcd.go:626","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-11-11T22:10:12.325231Z","caller":"embed/etcd.go:428","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [c7a84feba6a9] <==
{"level":"warn","ts":"2025-11-11T22:10:32.817837Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34180","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.822024Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.827306Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34232","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.830271Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.833031Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34264","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.835845Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34276","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.840078Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34296","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.843417Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34314","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.846414Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34332","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.849169Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34340","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.854709Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34362","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.857625Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34366","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.860371Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34382","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.863216Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34400","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.866125Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34410","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.870338Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34442","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.872312Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34460","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.875114Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34478","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.877975Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34494","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.890634Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34512","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.893558Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34526","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.896539Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34542","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.899624Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34572","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.902653Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34592","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.905589Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34612","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.908322Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34634","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.911085Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34648","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.915477Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34662","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.918628Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34684","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.921560Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34706","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.924421Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34724","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.927407Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.930117Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34742","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.932972Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34758","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.935815Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34770","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.938616Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34798","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.942320Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34822","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.945120Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34830","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.947951Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34852","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.950773Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34878","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.953661Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34900","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.956476Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34902","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.959212Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34918","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.962091Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34936","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.973868Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34958","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.976967Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34980","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.979632Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34984","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.982511Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35004","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.985271Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35016","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.988460Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35036","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.991435Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35054","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.994084Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35070","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.996958Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35092","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:32.999752Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35128","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.005805Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35138","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.008760Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35160","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.019930Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35174","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.022852Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35180","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.025754Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35196","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-11T22:10:33.040523Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35222","server-name":"","error":"EOF"}


==> kernel <==
 22:15:13 up  1:25,  0 users,  load average: 3.04, 3.00, 2.62
Linux minikube 6.10.14-linuxkit #1 SMP Wed Sep 10 06:47:45 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [050dfaa93ff2] <==
W1111 22:11:36.514480       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:36.514515       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1111 22:11:36.514542       1 handler_proxy.go:143] error resolving kube-system/metrics-server: service "metrics-server" not found
I1111 22:11:36.516417       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1111 22:11:36.521855       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1111 22:11:36.527715       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1111 22:11:36.572852       1 alloc.go:328] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.105.37.83"}
W1111 22:11:36.576483       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:36.576508       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W1111 22:11:36.581667       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:36.581686       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W1111 22:11:37.522411       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:37.522670       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I1111 22:11:37.522696       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1111 22:11:37.522580       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:37.522818       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1111 22:11:37.524407       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1111 22:11:38.080454       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W1111 22:11:47.555097       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:47.555141       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1111 22:11:47.555336       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.105.37.83:443: connect: connection refused" logger="UnhandledError"
E1111 22:11:47.556463       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.105.37.83:443: connect: connection refused" logger="UnhandledError"
E1111 22:11:47.562792       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.105.37.83:443: connect: connection refused" logger="UnhandledError"
W1111 22:11:48.555946       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:48.555987       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I1111 22:11:48.556000       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1111 22:11:48.556183       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:48.556213       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1111 22:11:48.558356       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1111 22:11:52.593557       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.105.37.83:443/apis/metrics.k8s.io/v1beta1\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
W1111 22:11:52.593838       1 handler_proxy.go:99] no RequestInfo found in the context
E1111 22:11:52.593937       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1111 22:11:52.610708       1 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1111 22:12:02.563025       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1111 22:12:44.341325       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1111 22:13:20.663896       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1111 22:13:54.703937       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1111 22:14:36.880058       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1111 22:15:12.920513       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [8c25250462eb] <==
W1111 22:10:11.083516       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:11.100872       1 logging.go:55] [core] [Channel #127 SubChannel #129]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:11.104755       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
{"level":"warn","ts":"2025-11-11T22:10:11.719366Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0x4000e263c0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""}
W1111 22:10:13.233613       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.241329       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.277411       1 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
{"level":"warn","ts":"2025-11-11T22:10:13.453905Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0x4000e263c0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""}
W1111 22:10:13.454851       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.464896       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.487958       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.490139       1 logging.go:55] [core] [Channel #27 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.545309       1 logging.go:55] [core] [Channel #199 SubChannel #201]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.700490       1 logging.go:55] [core] [Channel #143 SubChannel #145]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.702107       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
{"level":"warn","ts":"2025-11-11T22:10:13.723494Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0x4000faa1e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""}
W1111 22:10:13.764782       1 logging.go:55] [core] [Channel #87 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.786765       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.794746       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.852332       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.924334       1 logging.go:55] [core] [Channel #12 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.975738       1 logging.go:55] [core] [Channel #111 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:13.995211       1 logging.go:55] [core] [Channel #235 SubChannel #237]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.014896       1 logging.go:55] [core] [Channel #31 SubChannel #33]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.020742       1 logging.go:55] [core] [Channel #163 SubChannel #165]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.126169       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.159003       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.161436       1 logging.go:55] [core] [Channel #167 SubChannel #169]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.203834       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.271152       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.275629       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.289518       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.296476       1 logging.go:55] [core] [Channel #83 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.304476       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.347460       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.377979       1 logging.go:55] [core] [Channel #95 SubChannel #97]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.414354       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.455300       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.459752       1 logging.go:55] [core] [Channel #119 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.478742       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.515720       1 logging.go:55] [core] [Channel #21 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.520428       1 logging.go:55] [core] [Channel #227 SubChannel #229]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.543755       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.623384       1 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.623383       1 logging.go:55] [core] [Channel #207 SubChannel #209]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.627429       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.700456       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.778412       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.789942       1 logging.go:55] [core] [Channel #223 SubChannel #225]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.806416       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.836802       1 logging.go:55] [core] [Channel #151 SubChannel #153]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.891806       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.928743       1 logging.go:55] [core] [Channel #67 SubChannel #69]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:14.932409       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.054967       1 logging.go:55] [core] [Channel #191 SubChannel #193]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.177976       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.230209       1 logging.go:55] [core] [Channel #2 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.231076       1 logging.go:55] [core] [Channel #187 SubChannel #189]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.311213       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1111 22:10:15.380072       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [0d8a06bcd9b1] <==
I1111 22:10:54.446117       1 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I1111 22:10:54.497755       1 controllermanager.go:781] "Started controller" controller="service-cidr-controller"
I1111 22:10:54.497802       1 controllermanager.go:759] "Warning: skipping controller" controller="storage-version-migrator-controller"
I1111 22:10:54.499799       1 servicecidrs_controller.go:137] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I1111 22:10:54.499831       1 shared_informer.go:349] "Waiting for caches to sync" controller="service-cidr-controller"
I1111 22:10:54.504461       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1111 22:10:54.512589       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1111 22:10:54.516172       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1111 22:10:54.517059       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1111 22:10:54.517627       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1111 22:10:54.517787       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1111 22:10:54.518075       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1111 22:10:54.543746       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1111 22:10:54.543777       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1111 22:10:54.543825       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1111 22:10:54.543884       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1111 22:10:54.543916       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1111 22:10:54.546073       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1111 22:10:54.546322       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1111 22:10:54.547290       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1111 22:10:54.548802       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1111 22:10:54.550806       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1111 22:10:54.550840       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1111 22:10:54.550847       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1111 22:10:54.550851       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1111 22:10:54.560531       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1111 22:10:54.595755       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1111 22:10:54.595796       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1111 22:10:54.595832       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1111 22:10:54.595849       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1111 22:10:54.595883       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1111 22:10:54.595900       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1111 22:10:54.595917       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1111 22:10:54.595970       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1111 22:10:54.596557       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1111 22:10:54.598609       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1111 22:10:54.598681       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1111 22:10:54.598724       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1111 22:10:54.600379       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1111 22:10:54.600475       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1111 22:10:54.602745       1 shared_informer.go:356] "Caches are synced" controller="node"
I1111 22:10:54.602767       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1111 22:10:54.602771       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1111 22:10:54.602778       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1111 22:10:54.602782       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1111 22:10:54.602786       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1111 22:10:54.602934       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1111 22:10:54.604897       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1111 22:10:54.604919       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1111 22:10:54.605157       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1111 22:10:54.605174       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1111 22:10:54.606771       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1111 22:10:54.606806       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1111 22:10:54.606824       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1111 22:10:54.611769       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1111 22:10:54.612666       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1111 22:10:54.613814       1 shared_informer.go:356] "Caches are synced" controller="job"
I1111 22:10:54.615767       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1111 22:10:54.616793       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1111 22:10:54.618704       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [82be0dc63477] <==
I1111 22:10:18.372187       1 serving.go:386] Generated self-signed cert in-memory
I1111 22:10:19.219953       1 controllermanager.go:191] "Starting" version="v1.34.0"
I1111 22:10:19.219970       1 controllermanager.go:193] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1111 22:10:19.220723       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I1111 22:10:19.220762       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1111 22:10:19.220769       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1111 22:10:19.220761       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
E1111 22:10:29.225497       1 controllermanager.go:245] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://192.168.49.2:8443/healthz\": dial tcp 192.168.49.2:8443: connect: connection refused"


==> kube-proxy [04e6fd8c64cb] <==
I1111 22:03:58.467075       1 server_linux.go:53] "Using iptables proxy"
I1111 22:03:58.565569       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
E1111 22:04:00.181700       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes \"minikube\" is forbidden: User \"system:serviceaccount:kube-system:kube-proxy\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I1111 22:04:01.466176       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1111 22:04:01.466200       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1111 22:04:01.466249       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1111 22:04:01.475657       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1111 22:04:01.475690       1 server_linux.go:132] "Using iptables Proxier"
I1111 22:04:01.478237       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1111 22:04:01.478355       1 server.go:527] "Version info" version="v1.34.0"
I1111 22:04:01.478363       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1111 22:04:01.478907       1 config.go:200] "Starting service config controller"
I1111 22:04:01.478917       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1111 22:04:01.479184       1 config.go:309] "Starting node config controller"
I1111 22:04:01.479206       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1111 22:04:01.479212       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1111 22:04:01.479216       1 config.go:106] "Starting endpoint slice config controller"
I1111 22:04:01.479220       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1111 22:04:01.479232       1 config.go:403] "Starting serviceCIDR config controller"
I1111 22:04:01.479238       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1111 22:04:01.578985       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1111 22:04:01.579961       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1111 22:04:01.579987       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [d965ca2bf5d0] <==
I1111 22:10:17.979796       1 server_linux.go:53] "Using iptables proxy"
I1111 22:10:18.086639       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
E1111 22:10:18.087230       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:19.320996       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:22.031233       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:26.468048       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I1111 22:10:38.387748       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1111 22:10:38.387825       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1111 22:10:38.388009       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1111 22:10:38.412590       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1111 22:10:38.412669       1 server_linux.go:132] "Using iptables Proxier"
I1111 22:10:38.416639       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1111 22:10:38.416908       1 server.go:527] "Version info" version="v1.34.0"
I1111 22:10:38.416929       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1111 22:10:38.418558       1 config.go:200] "Starting service config controller"
I1111 22:10:38.418604       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1111 22:10:38.418627       1 config.go:309] "Starting node config controller"
I1111 22:10:38.418635       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1111 22:10:38.418694       1 config.go:403] "Starting serviceCIDR config controller"
I1111 22:10:38.418703       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1111 22:10:38.418728       1 config.go:106] "Starting endpoint slice config controller"
I1111 22:10:38.418735       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1111 22:10:38.519593       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1111 22:10:38.519614       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1111 22:10:38.519643       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1111 22:10:38.519657       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [3887ff7c1c5a] <==
I1111 22:03:59.095425       1 serving.go:386] Generated self-signed cert in-memory
W1111 22:04:00.100188       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1111 22:04:00.100215       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1111 22:04:00.100223       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1111 22:04:00.100230       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1111 22:04:00.181259       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1111 22:04:00.181281       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1111 22:04:00.182762       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1111 22:04:00.182801       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1111 22:04:00.182817       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1111 22:04:00.182829       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1111 22:04:00.185832       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1111 22:04:00.185955       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1111 22:04:00.186006       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1111 22:04:00.186033       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1111 22:04:00.186050       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1111 22:04:00.186060       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1111 22:04:00.261646       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:04:00.261700       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1111 22:04:00.261720       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1111 22:04:00.261789       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1111 22:04:00.261819       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1111 22:04:00.261963       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1111 22:04:00.262012       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1111 22:04:00.262190       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1111 22:04:00.262331       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1111 22:04:00.262542       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1111 22:04:00.263333       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1111 22:04:00.263972       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
I1111 22:04:00.283081       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1111 22:10:05.382684       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1111 22:10:05.458986       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1111 22:10:05.459603       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I1111 22:10:05.459635       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I1111 22:10:05.467607       1 server.go:265] "[graceful-termination] secure server is exiting"
E1111 22:10:05.467669       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [d737346b8206] <==
E1111 22:10:19.746076       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:19.765528       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1111 22:10:19.775943       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1111 22:10:19.784747       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1111 22:10:19.784755       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1111 22:10:19.838770       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceslices?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1111 22:10:19.910543       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1111 22:10:19.918451       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1111 22:10:19.949768       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1111 22:10:20.013349       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1111 22:10:20.024399       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1111 22:10:20.124704       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1111 22:10:20.132284       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E1111 22:10:20.210175       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1111 22:10:20.214901       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1111 22:10:20.241635       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1111 22:10:20.444002       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1111 22:10:20.492243       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/deviceclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1111 22:10:20.513001       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1111 22:10:21.734669       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1111 22:10:21.792635       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1111 22:10:21.820830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1111 22:10:21.824423       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1111 22:10:21.839565       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1111 22:10:21.892927       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E1111 22:10:21.963136       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1111 22:10:22.098404       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1111 22:10:22.121696       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1111 22:10:22.311252       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1111 22:10:22.410909       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:22.488718       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceslices?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1111 22:10:22.650136       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1111 22:10:22.666075       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1111 22:10:22.789906       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1111 22:10:22.880473       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1111 22:10:23.000897       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1111 22:10:23.113368       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/deviceclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1111 22:10:23.252476       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1111 22:10:25.228197       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1111 22:10:25.749252       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1111 22:10:25.787862       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1111 22:10:25.791662       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1111 22:10:25.822471       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1111 22:10:25.917607       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1111 22:10:26.072566       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1111 22:10:26.494927       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1111 22:10:26.714313       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1111 22:10:26.731607       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceslices?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1111 22:10:26.818481       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E1111 22:10:27.000465       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1111 22:10:27.153708       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1111 22:10:27.227315       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1111 22:10:27.749813       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1111 22:10:28.076195       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/deviceclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1111 22:10:28.399422       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1111 22:10:28.563955       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1111 22:10:28.923100       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1111 22:10:32.307836       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1111 22:10:33.258747       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
I1111 22:10:38.424770       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 11 22:10:33 minikube kubelet[2248]: I1111 22:10:33.527713    2248 scope.go:117] "RemoveContainer" containerID="3b93af034a062028a6f99f51339ddbc7786a3a073a441ebdb1517b340d770825"
Nov 11 22:10:33 minikube kubelet[2248]: E1111 22:10:33.527866    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:10:33 minikube kubelet[2248]: E1111 22:10:33.527880    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-855c9754f9-bpqz5_kubernetes-dashboard(95fa25a0-f770-491b-8e2f-1e9570d79a08)\"" pod="kubernetes-dashboard/kubernetes-dashboard-855c9754f9-bpqz5" podUID="95fa25a0-f770-491b-8e2f-1e9570d79a08"
Nov 11 22:10:36 minikube kubelet[2248]: I1111 22:10:36.724232    2248 scope.go:117] "RemoveContainer" containerID="82be0dc63477ad9efaaf93e2fd77a36a2b6f7ec99a6f12c3eea1066506ea4880"
Nov 11 22:10:36 minikube kubelet[2248]: E1111 22:10:36.724688    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(3b51c8241e224d47681cce32ea99b407)\"" pod="kube-system/kube-controller-manager-minikube" podUID="3b51c8241e224d47681cce32ea99b407"
Nov 11 22:10:41 minikube kubelet[2248]: E1111 22:10:41.527813    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:10:44 minikube kubelet[2248]: I1111 22:10:44.528407    2248 scope.go:117] "RemoveContainer" containerID="3b93af034a062028a6f99f51339ddbc7786a3a073a441ebdb1517b340d770825"
Nov 11 22:10:44 minikube kubelet[2248]: E1111 22:10:44.528798    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:10:45 minikube kubelet[2248]: I1111 22:10:45.527326    2248 scope.go:117] "RemoveContainer" containerID="a3390aa677aeb81813052992ce150d586827d792223a7c417a9711ec33e74eb5"
Nov 11 22:10:52 minikube kubelet[2248]: I1111 22:10:52.528449    2248 scope.go:117] "RemoveContainer" containerID="82be0dc63477ad9efaaf93e2fd77a36a2b6f7ec99a6f12c3eea1066506ea4880"
Nov 11 22:10:55 minikube kubelet[2248]: E1111 22:10:55.529355    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:10:55 minikube kubelet[2248]: E1111 22:10:55.529350    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:11:09 minikube kubelet[2248]: E1111 22:11:09.532063    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:11:10 minikube kubelet[2248]: E1111 22:11:10.533765    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:11:23 minikube kubelet[2248]: E1111 22:11:23.529095    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:11:23 minikube kubelet[2248]: E1111 22:11:23.529939    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:11:34 minikube kubelet[2248]: E1111 22:11:34.531512    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:11:36 minikube kubelet[2248]: I1111 22:11:36.719501    2248 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/7041af85-7ec6-4662-8995-f3fed310bcb1-tmp-dir\") pod \"metrics-server-85b7d694d7-d62gq\" (UID: \"7041af85-7ec6-4662-8995-f3fed310bcb1\") " pod="kube-system/metrics-server-85b7d694d7-d62gq"
Nov 11 22:11:36 minikube kubelet[2248]: I1111 22:11:36.719543    2248 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v96ql\" (UniqueName: \"kubernetes.io/projected/7041af85-7ec6-4662-8995-f3fed310bcb1-kube-api-access-v96ql\") pod \"metrics-server-85b7d694d7-d62gq\" (UID: \"7041af85-7ec6-4662-8995-f3fed310bcb1\") " pod="kube-system/metrics-server-85b7d694d7-d62gq"
Nov 11 22:11:38 minikube kubelet[2248]: E1111 22:11:38.529239    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:11:47 minikube kubelet[2248]: I1111 22:11:47.550672    2248 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/metrics-server-85b7d694d7-d62gq" podStartSLOduration=2.7730603780000003 podStartE2EDuration="11.550651424s" podCreationTimestamp="2025-11-11 22:11:36 +0000 UTC" firstStartedPulling="2025-11-11 22:11:37.016874961 +0000 UTC m=+599.531473773" lastFinishedPulling="2025-11-11 22:11:45.794465965 +0000 UTC m=+608.309064819" observedRunningTime="2025-11-11 22:11:46.541954382 +0000 UTC m=+609.056553278" watchObservedRunningTime="2025-11-11 22:11:47.550651424 +0000 UTC m=+610.065250236"
Nov 11 22:11:48 minikube kubelet[2248]: E1111 22:11:48.528045    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:11:52 minikube kubelet[2248]: E1111 22:11:52.530232    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:12:00 minikube kubelet[2248]: E1111 22:12:00.530065    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:12:05 minikube kubelet[2248]: E1111 22:12:05.529932    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:12:12 minikube kubelet[2248]: E1111 22:12:12.530144    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:12:20 minikube kubelet[2248]: E1111 22:12:20.528463    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:12:23 minikube kubelet[2248]: E1111 22:12:23.529899    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:12:35 minikube kubelet[2248]: E1111 22:12:35.529777    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:12:38 minikube kubelet[2248]: E1111 22:12:38.529275    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:12:49 minikube kubelet[2248]: E1111 22:12:49.531065    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:12:50 minikube kubelet[2248]: E1111 22:12:50.531609    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:13:02 minikube kubelet[2248]: E1111 22:13:02.528819    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:13:03 minikube kubelet[2248]: E1111 22:13:03.530378    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:13:13 minikube kubelet[2248]: E1111 22:13:13.530204    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:13:18 minikube kubelet[2248]: E1111 22:13:18.528394    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:13:24 minikube kubelet[2248]: E1111 22:13:24.529433    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:13:31 minikube kubelet[2248]: E1111 22:13:31.532043    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:13:39 minikube kubelet[2248]: E1111 22:13:39.530867    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:13:42 minikube kubelet[2248]: E1111 22:13:42.533062    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:13:52 minikube kubelet[2248]: E1111 22:13:52.530067    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:13:56 minikube kubelet[2248]: E1111 22:13:56.529968    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:14:06 minikube kubelet[2248]: E1111 22:14:06.530468    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:14:07 minikube kubelet[2248]: E1111 22:14:07.530570    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:14:19 minikube kubelet[2248]: E1111 22:14:19.530920    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:14:21 minikube kubelet[2248]: E1111 22:14:21.529764    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:14:30 minikube kubelet[2248]: E1111 22:14:30.528895    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:14:32 minikube kubelet[2248]: E1111 22:14:32.529320    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:14:41 minikube kubelet[2248]: E1111 22:14:41.531033    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:14:45 minikube kubelet[2248]: E1111 22:14:45.530814    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:14:53 minikube kubelet[2248]: E1111 22:14:53.601688    2248 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="react-app:latest"
Nov 11 22:14:53 minikube kubelet[2248]: E1111 22:14:53.601832    2248 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="react-app:latest"
Nov 11 22:14:53 minikube kubelet[2248]: E1111 22:14:53.602021    2248 kuberuntime_manager.go:1449] "Unhandled Error" err="container react-app start failed in pod react-app-55f69d4ddc-gpk2n_default(48ee4f5a-3a72-41f9-88ca-a9dd6059a044): ErrImagePull: Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 11 22:14:53 minikube kubelet[2248]: E1111 22:14:53.602096    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ErrImagePull: \"Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:15:00 minikube kubelet[2248]: E1111 22:15:00.472706    2248 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="react-app:latest"
Nov 11 22:15:00 minikube kubelet[2248]: E1111 22:15:00.472842    2248 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="react-app:latest"
Nov 11 22:15:00 minikube kubelet[2248]: E1111 22:15:00.473002    2248 kuberuntime_manager.go:1449] "Unhandled Error" err="container react-app start failed in pod react-app-55f69d4ddc-ddlgz_default(d524aad7-d6a1-4ed4-b4b4-010cba4943ae): ErrImagePull: Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 11 22:15:00 minikube kubelet[2248]: E1111 22:15:00.473069    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ErrImagePull: \"Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"
Nov 11 22:15:07 minikube kubelet[2248]: E1111 22:15:07.532565    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/react-app-55f69d4ddc-gpk2n" podUID="48ee4f5a-3a72-41f9-88ca-a9dd6059a044"
Nov 11 22:15:13 minikube kubelet[2248]: E1111 22:15:13.531159    2248 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"react-app\" with ImagePullBackOff: \"Back-off pulling image \\\"react-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for react-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/react-app-55f69d4ddc-ddlgz" podUID="d524aad7-d6a1-4ed4-b4b4-010cba4943ae"


==> kubernetes-dashboard [3b93af034a06] <==
2025/11/11 22:10:18 Using namespace: kubernetes-dashboard
2025/11/11 22:10:18 Using in-cluster config to connect to apiserver
2025/11/11 22:10:18 Using secret token for csrf signing
2025/11/11 22:10:18 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/11/11 22:10:18 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0x40006dfa78)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x2c0
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0x400050c300)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x7c
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x14957f0?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x30
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1bc


==> kubernetes-dashboard [d860c19805d4] <==
2025/11/11 22:12:08 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:08 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:08 [2025-11-11T22:12:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:08 received 0 resources from sidecar instead of 2
2025/11/11 22:12:08 [2025-11-11T22:12:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of namespaces
2025/11/11 22:12:13 Getting list of all jobs in the cluster
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all deployments in the cluster
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all cron jobs in the cluster
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all pods in the cluster
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all replica sets in the cluster
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all replication controllers in the cluster
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 Getting pod metrics
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/11/11 22:12:13 Getting list of all pet sets in the cluster
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:13 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:13 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:13 Skipping metric because of error: Metric label not set.
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:13 received 0 resources from sidecar instead of 2
2025/11/11 22:12:13 [2025-11-11T22:12:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Incoming HTTP/1.1 GET /api/v1/appdeployment/protocols request from 127.0.0.1: 
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/11/11 22:12:15 Getting list of namespaces
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:15 [2025-11-11T22:12:15Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:18 [2025-11-11T22:12:18Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/11/11 22:12:18 Getting list of namespaces
2025/11/11 22:12:18 [2025-11-11T22:12:18Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:23 [2025-11-11T22:12:23Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/11/11 22:12:23 Getting list of namespaces
2025/11/11 22:12:23 [2025-11-11T22:12:23Z] Outcoming response to 127.0.0.1 with 200 status code
2025/11/11 22:12:25 [2025-11-11T22:12:25Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/11/11 22:12:25 Getting list of namespaces
2025/11/11 22:12:25 [2025-11-11T22:12:25Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [83dbd999e7f0] <==
W1111 22:14:14.251852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:14.254885       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:16.260722       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:16.266775       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:18.273489       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:18.279563       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:20.286423       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:20.293169       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:22.301725       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:22.308901       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:24.315292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:24.321179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:26.328587       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:26.335034       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:28.341862       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:28.349186       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:30.355183       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:30.361422       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:32.368138       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:32.378755       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:34.384166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:34.391682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:36.398679       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:36.406499       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:38.414127       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:38.420591       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:40.427846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:40.435231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:42.441641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:42.447209       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:44.454458       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:44.460242       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:46.465664       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:46.471443       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:48.478893       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:48.484156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:50.490240       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:50.499090       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:52.504912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:52.510958       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:54.512874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:54.515760       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:56.518918       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:56.522454       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:58.530453       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:14:58.536380       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:00.542954       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:00.548120       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:02.554994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:02.561323       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:04.567647       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:04.575283       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:06.581875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:06.587089       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:08.595517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:08.601386       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:10.608551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:10.613561       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:12.619097       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:15:12.627220       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [a3390aa677ae] <==
W1111 22:09:06.672584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:06.679734       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:08.687022       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:08.694183       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:10.701244       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:10.707267       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:12.712405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:12.718136       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:14.722506       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:14.727636       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:16.731301       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:16.736576       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:18.741831       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:18.747329       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:20.753789       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:20.759790       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:22.767025       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:22.774448       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:24.781460       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:24.787186       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:26.794492       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:26.800757       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:28.807130       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:28.813189       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:30.821699       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:30.827660       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:32.834933       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:32.844350       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:34.851764       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:34.859253       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:36.866424       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:36.872268       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:38.878874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:38.886275       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:40.891514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:40.897569       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:42.903968       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:42.912370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:44.921784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:44.929115       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:46.934941       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:46.939033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:48.941432       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:48.945095       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:50.950805       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:50.957161       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:52.962993       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:52.972319       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:54.979205       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:54.985316       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:56.991792       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:57.000574       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:59.006655       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:09:59.012994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:01.017141       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:01.022951       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:03.028407       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:03.037737       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:05.045650       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1111 22:10:05.052700       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

